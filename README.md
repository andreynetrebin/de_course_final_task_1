# Проект: Анализ данных с использованием PySpark и ClickHouse
![de_course_task1](https://github.com/user-attachments/assets/88e99936-111f-4877-a9ba-e3aa499620e3)
## Содержание

- [Описание](#описание)
- [Задание](#задание)
- [Диаграммы](#диаграммы)

## Описание
Этот проект предназначен для анализа данных с использованием PySpark и ClickHouse. Он включает в себя загрузку данных, их обработку и анализ, а также загрузку результатов в базу данных ClickHouse. Проект организован с использованием Apache Airflow для управления задачами.

## Задание

1. Обратите внимание, что в проекте используются PySpark и ClickHouse. PySpark в Airflow в Docker Compose отсутствует, его необходимо добавить самостоятельно.
2. Предварительно разверните Docker Compose из первых шагов. Шаги 1-7 должны быть выполнены в DAG.
3. Загрузите файл данных в DataFrame PySpark. Обязательно выведите количество строк.
4. Убедитесь, что данные корректно прочитаны (правильный формат, отсутствие пустых строк).
5. Преобразуйте текстовые и числовые поля в соответствующие типы данных (например, дата, число).
6. Вычислите средний и медианный год постройки зданий.
7. Определите топ-10 областей и городов с наибольшим количеством объектов.
8. Найдите здания с максимальной и минимальной площадью в рамках каждой области.
9. Определите количество зданий по десятилетиям (например, сколько зданий построено в 1950-х, 1960-х и т.д.).
10. Создайте схему таблицы в ClickHouse, которая будет соответствовать структуре ваших данных. Это можно сделать не через Airflow.
11. Настройте соединение с ClickHouse из скрипта, учтите, что сделать это необходимо в Airflow.
12. Загрузите обработанные данные из DataFrame в таблицу в ClickHouse (Airflow).
13. Выполните SQL скрипт в Python, который выведет топ 25 домов, у которых площадь больше 60 кв.м (Airflow).

## Диаграммы
![de_course_task1_блок_схема](https://github.com/user-attachments/assets/901ce075-a80b-4b9a-8d73-2da18e465adf)
